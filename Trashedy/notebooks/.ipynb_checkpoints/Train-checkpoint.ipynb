{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b362174",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import csv\n",
    "random.seed(4)\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "# Non propre A DELETE\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "# ENDELETE\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.transforms.functional as F\n",
    "import torch.nn.functional as func\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.io import read_image\n",
    "from pathlib import Path\n",
    "import albumentations as A\n",
    "## A nettoyer\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "## En evaluation\n",
    "import helpers.engine as eng # Needs debugging\n",
    "from helpers.plotting import *\n",
    "#Temporary\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27cc6de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINITION DES FONCTIONS ###\n",
    "class ObjectDetectionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset Class to facilitate loading data for the Object Detection Task\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 path_to_dataset,  \n",
    "                 mapping = None, \n",
    "                 mode = 'train', \n",
    "                 transform = None): \n",
    "        \"\"\" \n",
    "        Args:\n",
    "            annotations: The path to the annotations CSV file. Format: file_name, classes, xmin, ymin, xmax, ymax\n",
    "            train_test_valid_split: The path to the tags CSV file for train, test, valid split. Format: file_name, tag\n",
    "            mapping: a dictionary containing mapping of class name and class index. Format : {'class_name' : 'class_index'}, Default: None\n",
    "            mode: Mode in which to instantiate class. Default: 'train'\n",
    "            transform: The transforms to be applied to the image data\n",
    "\n",
    "        Returns:\n",
    "            image : Torch Tensor, target: Torch Tensor, file_name : str\n",
    "        \"\"\"\n",
    "        self.mapping = mapping\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "\n",
    "        self.path_to_images = path_to_dataset\n",
    "        # Loading the annotation file (same format as Remo's)\n",
    "        \n",
    "        #my_data = pd.read_csv(annotations)\n",
    "        \n",
    "        # Here we append the file path to the filename. \n",
    "        # If dataset.export_annotations_to_file was used to create the annotation file, it would feature by default image file paths\n",
    "        \n",
    "        #my_data['file_name'] = my_data['file_name'].apply(lambda x : os.path.abspath(f'{self.path_to_images}{x}'))\n",
    "        #my_data = my_data.set_index('file_name')\n",
    "        \n",
    "        annotations_file_path = os.path.join(path_to_dataset, 'annotations.json')\n",
    "        data = read_file(annotations_file_path)\n",
    "        my_data  = extractDataSetFromCOCO(data,path_to_dataset)\n",
    "        my_data['file_name'] = path_to_dataset +  my_data['file_name']\n",
    "        # Loading the train/test split file (same format as Remo's)\n",
    "        #my_data['tag'] = pd.read_csv(train_test_valid_split, index_col='file_name')\n",
    "        \n",
    "        #my_data = my_data.reset_index()\n",
    "        # Load only Train/Test/Split depending on the mode\n",
    "        #my_data = my_data.loc[my_data['tag'] == mode].reset_index(drop=True)\n",
    "        self.data = my_data\n",
    "\n",
    "        self.file_names = self.data['file_name'].unique()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.file_names.shape[0]\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "\n",
    "        file_name = self.file_names[index]\n",
    "        records = self.data[self.data['file_name'] == file_name].reset_index()       \n",
    "        image = np.array(Image.open(file_name), dtype=np.float32)\n",
    "        image /= 255.0\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)  \n",
    "\n",
    "        # here we are assuming we don't have labels for the test set\n",
    "        if self.mode != 'test':\n",
    "            boxes = records[['xmin', 'ymin', 'xmax', 'ymax']].values\n",
    "            area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "            area = torch.as_tensor(area, dtype=torch.float32)\n",
    "\n",
    "            if self.mapping is not None:\n",
    "                labels = np.zeros((records.shape[0],))\n",
    "\n",
    "                for i in range(records.shape[0]):\n",
    "                    labels[i] = self.mapping[records.loc[i, 'classes']]\n",
    "\n",
    "                labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "\n",
    "            else:\n",
    "                labels = torch.ones((records.shape[0],), dtype=torch.int64)\n",
    "\n",
    "            iscrowd = torch.zeros((records.shape[0],), dtype=torch.int64)\n",
    "\n",
    "            target = {}\n",
    "            target['boxes'] = boxes\n",
    "            target['labels'] = labels\n",
    "            target['image_id'] = torch.tensor([index])\n",
    "            target['area'] = area\n",
    "            target['iscrowd'] = iscrowd \n",
    "            target['boxes'] = torch.stack(list((map(torch.tensor, target['boxes'])))).type(torch.float32)\n",
    "\n",
    "            return image, target, file_name\n",
    "        else:\n",
    "            return image, file_name\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "def read_file(path_to_file):\n",
    "    # Read annotations\n",
    "    with open(path_to_file, 'r') as f:\n",
    "        dataset = json.loads(f.read())\n",
    "    return dataset\n",
    "\n",
    "def extractDataSetFromCOCO(dataset,imagePath):\n",
    "    \"\"\" \n",
    "    Args:\n",
    "        dataset : a parsed JSON file countaining the annotations, must be COCO format.\n",
    "        imagePath : Path to a file with all the dataSet images.\n",
    "    Returns:\n",
    "        df : A pandas dataFrame with the annotations and the image file name\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame()\n",
    "    categories = [row['name'] for row in dataset['categories']]\n",
    "    images = [row['file_name'] for row in dataset['images']]\n",
    "    df['classes'] = [row['category_id'] for row in dataset['annotations']]\n",
    "    df['file_name'] = [row['image_id'] for row in dataset['annotations']]\n",
    "    df['file_name'] = [images[i] for i in df['file_name']]\n",
    "    df['classes'] = [categories[i] for i in df['classes']]\n",
    "    ## A DELETE ALED\n",
    "    #df['image'] = [ img_to_array(load_img(imagePath + '/' + fil, target_size=(224, 224))) for fil in df['file_name']] #Très sale, à voir pour faire mieux\n",
    "    \n",
    "    df['xmin'] = [row['bbox'][0] for row in dataset['annotations']]\n",
    "    df['ymin'] = [row['bbox'][1] for row in dataset['annotations']]\n",
    "    df['xmax'] = [row['bbox'][0]+row['bbox'][2] for row in dataset['annotations']]\n",
    "    df['ymax'] = [row['bbox'][1]+row['bbox'][3] for row in dataset['annotations']]\n",
    "    return df\n",
    "\n",
    "def evaluate(data_loader,device,cat_to_index): \n",
    "    \"\"\" \n",
    "    Args:\n",
    "        data_loader : A Pytorch dataloader with the images we want to perform detection on.\n",
    "        device : A Pytorch device, CPU or GPU.\n",
    "        cat_to_index : A dictionnary linking labels to a int.\n",
    "    Returns:\n",
    "        results : List of dictionnary with the bounding boxes, the labels and the scores. It can be easely saved as csv or json.\n",
    "    \"\"\"\n",
    "    mapping = { value : key for (key, value) in cat_to_index.items()}\n",
    "    detection_threshold = 0.3\n",
    "    results = []\n",
    "    model.eval()\n",
    "    data_loader = tqdm.tqdm(data_loader)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, image_ids in data_loader:\n",
    "\n",
    "            images = list(image.to(device) for image in images)\n",
    "            outputs = model(images)\n",
    "            for i, image in enumerate(images):\n",
    "\n",
    "                boxes = outputs[i]['boxes'].data.cpu().numpy()\n",
    "                scores = outputs[i]['scores'].data.cpu().numpy()\n",
    "                boxes = boxes[scores >= detection_threshold].astype(np.int32)\n",
    "                scores = scores[scores >= detection_threshold]\n",
    "                image_id = image_ids[i]\n",
    "\n",
    "                for box, labels,score in zip(boxes, outputs[i]['labels'],scores):\n",
    "                    results.append({'file_name' : os.path.basename(image_id), \n",
    "                                    'classes'   : mapping[labels.item()], \n",
    "                                    'xmin'      : box[0],\n",
    "                                    'ymin'      : box[1],\n",
    "                                    'xmax'      : box[2],\n",
    "                                    'ymax'      : box[3],\n",
    "                                    'scores'    : score})\n",
    "    return results\n",
    "\n",
    "def fastrcnn_loss_rebranded(class_logits, box_regression, labels, regression_targets):\n",
    "    # type: (Tensor, Tensor, List[Tensor], List[Tensor]) -> Tuple[Tensor, Tensor]\n",
    "    \"\"\"\n",
    "    Computes the loss for Faster R-CNN.\n",
    "    Args:\n",
    "        class_logits (Tensor)\n",
    "        box_regression (Tensor)\n",
    "        labels (list[BoxList])\n",
    "        regression_targets (Tensor)\n",
    "    Returns:\n",
    "        classification_loss (Tensor)\n",
    "        box_loss (Tensor)\n",
    "    \"\"\"\n",
    "\n",
    "    labels = torch.cat(labels, dim=0)\n",
    "    regression_targets = torch.cat(regression_targets, dim=0)\n",
    "\n",
    "    classification_loss = func.cross_entropy(class_logits, labels,weight = AA_WEIGHTS_ZZ)\n",
    "\n",
    "    # get indices that correspond to the regression targets for\n",
    "    # the corresponding ground truth labels, to be used with\n",
    "    # advanced indexing\n",
    "    sampled_pos_inds_subset = torch.where(labels > 0)[0]\n",
    "    labels_pos = labels[sampled_pos_inds_subset]\n",
    "    N, num_classes = class_logits.shape\n",
    "    box_regression = box_regression.reshape(N, box_regression.size(-1) // 4, 4)\n",
    "\n",
    "    box_loss = func.smooth_l1_loss(\n",
    "        box_regression[sampled_pos_inds_subset, labels_pos],\n",
    "        regression_targets[sampled_pos_inds_subset],\n",
    "        beta=1 / 9,\n",
    "        reduction='sum',\n",
    "    )\n",
    "    box_loss = box_loss / labels.numel()\n",
    "\n",
    "    return classification_loss, box_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5088a401",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONFIG ###\n",
    "\n",
    "model_path = \"../saved_models/model3SGD\"\n",
    "path_to_data = '../dataset/dataset_Trashedy_1.0/train/'\n",
    "path_to_validation_data = '../dataset/dataset_Trashedy_1.0/valid/'\n",
    "model_save_name = \"model_SGD_balanced\"\n",
    "device      = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "cat_to_index = {'other': 1, \n",
    "                'pet': 2,\n",
    "                'eps-polystyrene-': 3, \n",
    "                'plastic_bag' : 4 \n",
    "                }\n",
    "device      = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "num_classes = 5\n",
    "loss_value  = 0.0\n",
    "num_epochs  = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eade7729",
   "metadata": {},
   "outputs": [],
   "source": [
    "### LOAD DATASET ###\n",
    "\n",
    "tensor_transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "train_dataset = ObjectDetectionDataset(path_to_dataset = path_to_data,  \n",
    "                                       transform = tensor_transform,\n",
    "                                       mapping = cat_to_index,\n",
    "                                       mode = 'train')\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=1, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "validation_dataset = ObjectDetectionDataset(path_to_dataset = path_to_validation_data,  \n",
    "                                       transform = tensor_transform,\n",
    "                                       mapping = cat_to_index,\n",
    "                                       mode = 'valid')\n",
    "\n",
    "validation_data_loader = DataLoader(validation_dataset, batch_size=1, shuffle=False, num_workers=0, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8e29a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\diabo\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass classes=['eps-polystyrene-' 'other' 'pet' 'plastic_bag'], y=['other' 'plastic_bag' 'other' 'other' 'pet' 'pet' 'pet' 'pet' 'pet'\n",
      " 'other' 'plastic_bag' 'pet' 'pet' 'plastic_bag' 'pet' 'other' 'other'\n",
      " 'plastic_bag' 'other' 'plastic_bag' 'other' 'other' 'pet' 'pet' 'pet'\n",
      " 'other' 'other' 'plastic_bag' 'pet' 'plastic_bag' 'eps-polystyrene-'\n",
      " 'other' 'other' 'other' 'other' 'other' 'other' 'other' 'other' 'other'\n",
      " 'other' 'other' 'other' 'other' 'other' 'other' 'other' 'plastic_bag'\n",
      " 'pet' 'pet' 'other' 'pet' 'pet' 'pet' 'pet' 'pet' 'pet' 'pet' 'pet' 'pet'\n",
      " 'pet' 'pet' 'pet' 'pet' 'pet' 'pet' 'pet' 'other' 'other' 'other' 'other'\n",
      " 'other' 'other' 'other' 'other' 'other' 'other' 'other' 'other' 'other'\n",
      " 'pet' 'pet' 'pet' 'pet' 'pet' 'pet' 'pet' 'pet' 'pet' 'pet' 'pet'\n",
      " 'plastic_bag' 'other' 'other' 'other' 'other' 'other' 'other' 'other'\n",
      " 'other' 'other' 'other' 'other' 'other' 'other' 'other' 'other' 'other'\n",
      " 'other' 'other' 'other' 'other' 'other' 'other' 'other' 'other' 'other'\n",
      " 'pet' 'pet' 'other' 'other' 'other'] as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced', np.unique(validation_dataset.data['classes'].to_numpy()),validation_dataset.data['classes'].to_numpy())\n",
    "class_weights = np.insert(class_weights, 0,np.mean(class_weights) ) #Inserting the weight for the background, BUT HOW TO CHOOSE IT ????\n",
    "AA_WEIGHTS_ZZ = torch.from_numpy(class_weights)\n",
    "AA_WEIGHTS_ZZ = AA_WEIGHTS_ZZ.to(torch.float32)\n",
    "torchvision.models.detection.roi_heads.fastrcnn_loss = fastrcnn_loss_rebranded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52d7f1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TELECHARGEMENT DU MODELE ###\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005) # IMPROVE WITH ADAM !\n",
    "#optimizer = torch.optim.Adam(params, lr=0.005, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a12b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/79 [00:00<?, ?it/s]C:\\Users\\diabo\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      " 34%|████████████████████████████                                                      | 27/79 [07:03<13:59, 16.14s/it]"
     ]
    }
   ],
   "source": [
    "### ENTRAINEMENT DU MODELE ###\n",
    "\n",
    "l_losses = {'loss_classifier' : [],'loss_box_reg' : [],'loss_rpn_box_reg' : [],'loss_objectness':[]}\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    train_data_loader = tqdm.tqdm(train_data_loader)\n",
    "    for images, targets, image_ids in train_data_loader:\n",
    "\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        l_losses['loss_classifier'].append(loss_dict['loss_classifier'].item())\n",
    "        l_losses['loss_box_reg'].append(loss_dict['loss_box_reg'].item())\n",
    "        l_losses['loss_objectness'].append(loss_dict['loss_objectness'].item())\n",
    "        l_losses['loss_rpn_box_reg'].append(loss_dict['loss_rpn_box_reg'].item())\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step() \n",
    "    print('\\nTraining Loss : {:.5f}'.format(loss_value))\n",
    "    #eng.evaluate(model, validation_data_loader, device=device)\n",
    "\n",
    "## sauvegarde du modele et de ses performances\n",
    "os.mkdir(\"../saved_models/\" + model_save_name)\n",
    "torch.save(model,\"../saved_models/\" + model_save_name + \"/\" + model_save_name)\n",
    "\n",
    "with open('../saved_models/mydata.json', 'w') as f:\n",
    "    json.dump(l_losses, f)\n",
    "##\n",
    "\n",
    "print(\"Fin de l'entrainement, modele sauvegardé\")\n",
    "\n",
    "plot_loss_summary(l_losses,train_dataset.__len__(),num_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
