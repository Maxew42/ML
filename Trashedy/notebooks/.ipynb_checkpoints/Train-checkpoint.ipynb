{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b362174",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import csv\n",
    "random.seed(4)\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "# Non propre A DELETE\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "# ENDELETE\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.transforms.functional as F\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.io import read_image\n",
    "from pathlib import Path\n",
    "import albumentations as A\n",
    "## A nettoyer\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "## En evaluation\n",
    "import helpers.engine as eng\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a0c551c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINITION DES FONCTIONS ###\n",
    "class ObjectDetectionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset Class to facilitate loading data for the Object Detection Task\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 annotations,  \n",
    "                 mapping = None, \n",
    "                 mode = 'train', \n",
    "                 transform = None,path_to_images = './data2/train/'): \n",
    "        \"\"\" \n",
    "        Args:\n",
    "            annotations: The path to the annotations CSV file. Format: file_name, classes, xmin, ymin, xmax, ymax\n",
    "            train_test_valid_split: The path to the tags CSV file for train, test, valid split. Format: file_name, tag\n",
    "            mapping: a dictionary containing mapping of class name and class index. Format : {'class_name' : 'class_index'}, Default: None\n",
    "            mode: Mode in which to instantiate class. Default: 'train'\n",
    "            transform: The transforms to be applied to the image data\n",
    "\n",
    "        Returns:\n",
    "            image : Torch Tensor, target: Torch Tensor, file_name : str\n",
    "        \"\"\"\n",
    "        self.mapping = mapping\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "\n",
    "        self.path_to_images = path_to_images\n",
    "        # Loading the annotation file (same format as Remo's)\n",
    "        \n",
    "        #my_data = pd.read_csv(annotations)\n",
    "        \n",
    "        # Here we append the file path to the filename. \n",
    "        # If dataset.export_annotations_to_file was used to create the annotation file, it would feature by default image file paths\n",
    "        \n",
    "        #my_data['file_name'] = my_data['file_name'].apply(lambda x : os.path.abspath(f'{self.path_to_images}{x}'))\n",
    "        #my_data = my_data.set_index('file_name')\n",
    "        data = read_file(annotations)\n",
    "        my_data  = extractDataSetFromCOCO(data,path_to_images)\n",
    "        my_data['file_name'] = path_to_images +  my_data['file_name']\n",
    "        # Loading the train/test split file (same format as Remo's)\n",
    "        #my_data['tag'] = pd.read_csv(train_test_valid_split, index_col='file_name')\n",
    "        \n",
    "        #my_data = my_data.reset_index()\n",
    "        # Load only Train/Test/Split depending on the mode\n",
    "        #my_data = my_data.loc[my_data['tag'] == mode].reset_index(drop=True)\n",
    "        self.data = my_data\n",
    "\n",
    "        self.file_names = self.data['file_name'].unique()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.file_names.shape[0]\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "\n",
    "        file_name = self.file_names[index]\n",
    "        records = self.data[self.data['file_name'] == file_name].reset_index()       \n",
    "        image = np.array(Image.open(file_name), dtype=np.float32)\n",
    "        image /= 255.0\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)  \n",
    "\n",
    "        # here we are assuming we don't have labels for the test set\n",
    "        if self.mode != 'test':\n",
    "            boxes = records[['xmin', 'ymin', 'xmax', 'ymax']].values\n",
    "            area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "            area = torch.as_tensor(area, dtype=torch.float32)\n",
    "\n",
    "            if self.mapping is not None:\n",
    "                labels = np.zeros((records.shape[0],))\n",
    "\n",
    "                for i in range(records.shape[0]):\n",
    "                    labels[i] = self.mapping[records.loc[i, 'classes']]\n",
    "\n",
    "                labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "\n",
    "            else:\n",
    "                labels = torch.ones((records.shape[0],), dtype=torch.int64)\n",
    "\n",
    "            iscrowd = torch.zeros((records.shape[0],), dtype=torch.int64)\n",
    "\n",
    "            target = {}\n",
    "            target['boxes'] = boxes\n",
    "            target['labels'] = labels\n",
    "            target['image_id'] = torch.tensor([index])\n",
    "            target['area'] = area\n",
    "            target['iscrowd'] = iscrowd \n",
    "            target['boxes'] = torch.stack(list((map(torch.tensor, target['boxes'])))).type(torch.float32)\n",
    "\n",
    "            return image, target, file_name\n",
    "        else:\n",
    "            return image, file_name\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "def read_file(path_to_file):\n",
    "    # Read annotations\n",
    "    with open(path_to_file, 'r') as f:\n",
    "        dataset = json.loads(f.read())\n",
    "    return dataset\n",
    "\n",
    "def extractDataSetFromCOCO(dataset,imagePath):\n",
    "    df = pd.DataFrame()\n",
    "    categories = [row['name'] for row in dataset['categories']]\n",
    "    images = [row['file_name'] for row in dataset['images']]\n",
    "    df['classes'] = [row['category_id'] for row in dataset['annotations']]\n",
    "    df['file_name'] = [row['image_id'] for row in dataset['annotations']]\n",
    "    df['file_name'] = [images[i] for i in df['file_name']]\n",
    "    df['classes'] = [categories[i] for i in df['classes']]\n",
    "    ## A DELETE ALED\n",
    "    df['image'] = [ img_to_array(load_img(imagePath + '/' + fil)) for fil in df['file_name']] #Très sale, à voir pour faire mieux\n",
    "    #, target_size=(224, 224)\n",
    "    \n",
    "    df['xmin'] = [row['bbox'][0] for row in dataset['annotations']]\n",
    "    df['ymin'] = [row['bbox'][1] for row in dataset['annotations']]\n",
    "    df['xmax'] = [row['bbox'][0]+row['bbox'][2] for row in dataset['annotations']]\n",
    "    df['ymax'] = [row['bbox'][1]+row['bbox'][3] for row in dataset['annotations']]\n",
    "    return df\n",
    "\n",
    "def evaluate(data_loader,device): \n",
    "    detection_threshold = 0.3\n",
    "    results = []\n",
    "    model.eval()\n",
    "    data_loader = tqdm.tqdm(data_loader)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, image_ids in data_loader:\n",
    "\n",
    "            images = list(image.to(device) for image in images)\n",
    "            outputs = model(images)\n",
    "\n",
    "            for i, image in enumerate(images):\n",
    "\n",
    "                boxes = outputs[i]['boxes'].data.cpu().numpy()\n",
    "                scores = outputs[i]['scores'].data.cpu().numpy()\n",
    "                boxes = boxes[scores >= detection_threshold].astype(np.int32)\n",
    "                scores = scores[scores >= detection_threshold]\n",
    "                image_id = image_ids[i]\n",
    "\n",
    "                for box, labels in zip(boxes, outputs[i]['labels']):\n",
    "                    results.append({'file_name' : os.path.basename(image_id), \n",
    "                                    'classes'   : mapping[labels.item()], \n",
    "                                    'xmin'      : box[0],\n",
    "                                    'ymin'      : box[1],\n",
    "                                    'xmax'      : box[2],\n",
    "                                    'ymax'      : box[3]})\n",
    "    return results\n",
    "\n",
    "def show(imgs):\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = img.detach()\n",
    "        img = F.to_pil_image(img)\n",
    "        axs[0, i].imshow(np.asarray(img))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "        \n",
    "def show_img(image_path,file_name): # A EDITER\n",
    "    img = read_image(image_path + file_name)\n",
    "    l_boxes = []\n",
    "    for row in results:\n",
    "        if row['file_name'] == file_name:\n",
    "            l_boxes.append([row['xmin'],row['ymin'],row['xmax'],row['ymax']])\n",
    "\n",
    "    boxes = torch.tensor(l_boxes, dtype=torch.float)\n",
    "    colors = [\"blue\"]\n",
    "    result = draw_bounding_boxes(img, boxes, width=5)\n",
    "    show(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0f2e1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINITION DES FONCTIONS ###\n",
    "class ObjectDetectionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset Class to facilitate loading data for the Object Detection Task\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 path_to_dataset,  \n",
    "                 mapping = None, \n",
    "                 mode = 'train', \n",
    "                 transform = None): \n",
    "        \"\"\" \n",
    "        Args:\n",
    "            annotations: The path to the annotations CSV file. Format: file_name, classes, xmin, ymin, xmax, ymax\n",
    "            train_test_valid_split: The path to the tags CSV file for train, test, valid split. Format: file_name, tag\n",
    "            mapping: a dictionary containing mapping of class name and class index. Format : {'class_name' : 'class_index'}, Default: None\n",
    "            mode: Mode in which to instantiate class. Default: 'train'\n",
    "            transform: The transforms to be applied to the image data\n",
    "\n",
    "        Returns:\n",
    "            image : Torch Tensor, target: Torch Tensor, file_name : str\n",
    "        \"\"\"\n",
    "        self.mapping = mapping\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "\n",
    "        self.path_to_images = path_to_dataset\n",
    "        # Loading the annotation file (same format as Remo's)\n",
    "        \n",
    "        #my_data = pd.read_csv(annotations)\n",
    "        \n",
    "        # Here we append the file path to the filename. \n",
    "        # If dataset.export_annotations_to_file was used to create the annotation file, it would feature by default image file paths\n",
    "        \n",
    "        #my_data['file_name'] = my_data['file_name'].apply(lambda x : os.path.abspath(f'{self.path_to_images}{x}'))\n",
    "        #my_data = my_data.set_index('file_name')\n",
    "        \n",
    "        annotations_file_path = os.path.join(path_to_dataset, 'annotations.json')\n",
    "        data = read_file(annotations_file_path)\n",
    "        my_data  = extractDataSetFromCOCO(data,path_to_dataset)\n",
    "        my_data['file_name'] = path_to_dataset +  my_data['file_name']\n",
    "        # Loading the train/test split file (same format as Remo's)\n",
    "        #my_data['tag'] = pd.read_csv(train_test_valid_split, index_col='file_name')\n",
    "        \n",
    "        #my_data = my_data.reset_index()\n",
    "        # Load only Train/Test/Split depending on the mode\n",
    "        #my_data = my_data.loc[my_data['tag'] == mode].reset_index(drop=True)\n",
    "        self.data = my_data\n",
    "\n",
    "        self.file_names = self.data['file_name'].unique()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.file_names.shape[0]\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "\n",
    "        file_name = self.file_names[index]\n",
    "        records = self.data[self.data['file_name'] == file_name].reset_index()       \n",
    "        image = np.array(Image.open(file_name), dtype=np.float32)\n",
    "        image /= 255.0\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)  \n",
    "\n",
    "        # here we are assuming we don't have labels for the test set\n",
    "        if self.mode != 'test':\n",
    "            boxes = records[['xmin', 'ymin', 'xmax', 'ymax']].values\n",
    "            area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "            area = torch.as_tensor(area, dtype=torch.float32)\n",
    "\n",
    "            if self.mapping is not None:\n",
    "                labels = np.zeros((records.shape[0],))\n",
    "\n",
    "                for i in range(records.shape[0]):\n",
    "                    labels[i] = self.mapping[records.loc[i, 'classes']]\n",
    "\n",
    "                labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "\n",
    "            else:\n",
    "                labels = torch.ones((records.shape[0],), dtype=torch.int64)\n",
    "\n",
    "            iscrowd = torch.zeros((records.shape[0],), dtype=torch.int64)\n",
    "\n",
    "            target = {}\n",
    "            target['boxes'] = boxes\n",
    "            target['labels'] = labels\n",
    "            target['image_id'] = torch.tensor([index])\n",
    "            target['area'] = area\n",
    "            target['iscrowd'] = iscrowd \n",
    "            target['boxes'] = torch.stack(list((map(torch.tensor, target['boxes'])))).type(torch.float32)\n",
    "\n",
    "            return image, target, file_name\n",
    "        else:\n",
    "            return image, file_name\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "def read_file(path_to_file):\n",
    "    # Read annotations\n",
    "    with open(path_to_file, 'r') as f:\n",
    "        dataset = json.loads(f.read())\n",
    "    return dataset\n",
    "\n",
    "def extractDataSetFromCOCO(dataset,imagePath):\n",
    "    \"\"\" \n",
    "    Args:\n",
    "        dataset : a parsed JSON file countaining the annotations, must be COCO format.\n",
    "        imagePath : Path to a file with all the dataSet images.\n",
    "    Returns:\n",
    "        df : A pandas dataFrame with the annotations and the image file name\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame()\n",
    "    categories = [row['name'] for row in dataset['categories']]\n",
    "    images = [row['file_name'] for row in dataset['images']]\n",
    "    df['classes'] = [row['category_id'] for row in dataset['annotations']]\n",
    "    df['file_name'] = [row['image_id'] for row in dataset['annotations']]\n",
    "    df['file_name'] = [images[i] for i in df['file_name']]\n",
    "    df['classes'] = [categories[i] for i in df['classes']]\n",
    "    ## A DELETE ALED\n",
    "    #df['image'] = [ img_to_array(load_img(imagePath + '/' + fil, target_size=(224, 224))) for fil in df['file_name']] #Très sale, à voir pour faire mieux\n",
    "    \n",
    "    df['xmin'] = [row['bbox'][0] for row in dataset['annotations']]\n",
    "    df['ymin'] = [row['bbox'][1] for row in dataset['annotations']]\n",
    "    df['xmax'] = [row['bbox'][0]+row['bbox'][2] for row in dataset['annotations']]\n",
    "    df['ymax'] = [row['bbox'][1]+row['bbox'][3] for row in dataset['annotations']]\n",
    "    return df\n",
    "\n",
    "def evaluate(data_loader,device,cat_to_index): \n",
    "    \"\"\" \n",
    "    Args:\n",
    "        data_loader : A Pytorch dataloader with the images we want to perform detection on.\n",
    "        device : A Pytorch device, CPU or GPU.\n",
    "        cat_to_index : A dictionnary linking labels to a int.\n",
    "    Returns:\n",
    "        results : List of dictionnary with the bounding boxes, the labels and the scores. It can be easely saved as csv or json.\n",
    "    \"\"\"\n",
    "    mapping = { value : key for (key, value) in cat_to_index.items()}\n",
    "    detection_threshold = 0.3\n",
    "    results = []\n",
    "    model.eval()\n",
    "    data_loader = tqdm.tqdm(data_loader)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, image_ids in data_loader:\n",
    "\n",
    "            images = list(image.to(device) for image in images)\n",
    "            outputs = model(images)\n",
    "            for i, image in enumerate(images):\n",
    "\n",
    "                boxes = outputs[i]['boxes'].data.cpu().numpy()\n",
    "                scores = outputs[i]['scores'].data.cpu().numpy()\n",
    "                boxes = boxes[scores >= detection_threshold].astype(np.int32)\n",
    "                scores = scores[scores >= detection_threshold]\n",
    "                image_id = image_ids[i]\n",
    "\n",
    "                for box, labels,score in zip(boxes, outputs[i]['labels'],scores):\n",
    "                    results.append({'file_name' : os.path.basename(image_id), \n",
    "                                    'classes'   : mapping[labels.item()], \n",
    "                                    'xmin'      : box[0],\n",
    "                                    'ymin'      : box[1],\n",
    "                                    'xmax'      : box[2],\n",
    "                                    'ymax'      : box[3],\n",
    "                                    'scores'    : score})\n",
    "    return results\n",
    "        \n",
    "def convert_points_to_box(points, color = 'red', alpha = 0.09):\n",
    "    '''\n",
    "        Helper Function : return two pyplot.Rectangle for the purpous of plotting bboxes. One is the border and one is the fill color.\n",
    "    '''\n",
    "    upper_left_point = (points[0], points[1])\n",
    "    width = points[2] - points[0]\n",
    "    height = points[3] - points[1]\n",
    "    return (plt.Rectangle(upper_left_point, width, height, ec=color,fc=color, alpha=1,facecolor='none',fill = False),plt.Rectangle(upper_left_point, width, height, ec=color,fc=color, alpha=alpha))\n",
    "\n",
    "def show_bounding_boxes(img,l_boxes,l_scores = None,labels = None,fontsize = 16):\n",
    "    '''\n",
    "        Show the bounding boxes in l_boxes for a specified image img in np.array format. \n",
    "    '''\n",
    "    fig = plt.figure(figsize=(25, 10))\n",
    "    colors = ['red','blue']\n",
    "    ax = plt.gca()\n",
    "    colors_idx = [random.randint(0, len(colors)-1) for i in l_boxes]\n",
    "    plt.imshow(img)\n",
    "    for i,box in enumerate(l_boxes):\n",
    "        contour,texture = convert_points_to_box(box, colors[colors_idx[i]], 0.1)\n",
    "        ax.add_patch(contour)\n",
    "        ax.add_patch(texture)\n",
    "        if labels and l_scores:\n",
    "            plt.text(box[0],box[1],labels[i] + \" : \"+ str(np.round_(100*l_scores[i], decimals=2))+ \" %\",fontsize=fontsize,color =colors[colors_idx[i]] )\n",
    "\n",
    "    ax.get_xaxis().set_ticks([])\n",
    "    ax.get_yaxis().set_ticks([])\n",
    "    plt.show()\n",
    "    \n",
    "def show_result(results,path_to_dataset,file_name):\n",
    "    '''\n",
    "        Show the bounding boxes in l_boxes for a specified image img in np.array format. \n",
    "    '''\n",
    "    img = plt.imread(path_to_dataset + \"/\" + file_name)\n",
    "    l_boxes = []\n",
    "    l_labels = []\n",
    "    l_scores = []\n",
    "    for row in results:\n",
    "        if row['file_name'] == file_name:\n",
    "            l_boxes.append([row['xmin'],row['ymin'],row['xmax'],row['ymax']])\n",
    "            l_labels.append(row['classes'])\n",
    "            l_scores.append(row['scores'])\n",
    "    print(\"Nombre de déchets identifiés : \", len(l_boxes))\n",
    "    show_bounding_boxes(img,l_boxes,l_scores = l_scores,labels = l_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5088a401",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONFIG ###\n",
    "\n",
    "model_path = \"../saved_models/model3SGD\"\n",
    "path_to_data = '../dataset/dataset_Trashedy_1.0/train/'\n",
    "path_to_validation_data = '../dataset/dataset_Trashedy_1.0/valid/'\n",
    "device      = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "cat_to_index = {'other': 1, \n",
    "                'pet': 2,\n",
    "                'eps-polystyrene-': 3, \n",
    "                'plastic_bag' : 4 \n",
    "                }\n",
    "device      = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "num_classes = 5\n",
    "loss_value  = 0.0\n",
    "num_epochs  = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eade7729",
   "metadata": {},
   "outputs": [],
   "source": [
    "### LOAD DATASET ###\n",
    "\n",
    "\n",
    "tensor_transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "train_dataset = ObjectDetectionDataset(path_to_dataset = path_to_data,  \n",
    "                                       transform = tensor_transform,\n",
    "                                       mapping = cat_to_index,\n",
    "                                       mode = 'train')\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=1, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "validation_dataset = ObjectDetectionDataset(path_to_dataset = path_to_validation_data,  \n",
    "                                       transform = tensor_transform,\n",
    "                                       mapping = cat_to_index,\n",
    "                                       mode = 'valid')\n",
    "\n",
    "validation_data_loader = DataLoader(validation_dataset, batch_size=1, shuffle=False, num_workers=0, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52d7f1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TELECHARGEMENT DU MODELE ###\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005) # IMPROVE WITH ADAM !\n",
    "#optimizer = torch.optim.Adam(params, lr=0.005, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecc94246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating index...\n",
      "index created!\n",
      "testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\maxim\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test:  [ 0/31]  eta: 0:05:47  model_time: 11.0801 (11.0801)  evaluator_time: 0.0000 (0.0000)  time: 11.2219  data: 0.1418\n",
      "testing\n",
      "testing\n",
      "testing\n",
      "testing\n",
      "testing\n",
      "testing\n",
      "testing\n",
      "testing\n",
      "testing\n",
      "testing\n",
      "testing\n",
      "testing\n",
      "testing\n",
      "testing\n",
      "testing\n",
      "testing\n",
      "testing\n",
      "testing\n",
      "testing\n",
      "testing\n",
      "testing\n",
      "testing\n",
      "testing\n",
      "testing\n",
      "testing\n",
      "testing\n",
      "testing\n",
      "testing\n",
      "testing\n",
      "testing\n",
      "Test:  [30/31]  eta: 0:00:09  model_time: 9.5540 (9.6640)  evaluator_time: 0.0020 (0.0058)  time: 9.6891  data: 0.0878\n",
      "Test: Total time: 0:05:02 (9.7716 s / it)\n",
      "Averaged stats: model_time: 9.5540 (9.6640)  evaluator_time: 0.0020 (0.0058)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.03s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.001\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.002\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.001\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.006\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.024\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.024\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<helpers.coco_eval.CocoEvaluator at 0x1d1aa0b1880>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng.evaluate(model, validation_data_loader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a12b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/79 [00:00<?, ?it/s]c:\\users\\maxim\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "  5%|████▏                                                                              | 4/79 [00:35<11:23,  9.11s/it]"
     ]
    }
   ],
   "source": [
    "### ENTRAINEMENT DU MODELE ###\n",
    "l_losses = {'loss_classifier' : [],'loss_box_reg' : []}\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    train_data_loader = tqdm.tqdm(train_data_loader)\n",
    "    for images, targets, image_ids in train_data_loader:\n",
    "\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        \n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "        #l_losses.append(losses.item())\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step() \n",
    "    print('\\nTraining Loss : {:.5f}'.format(loss_value))\n",
    "    eng.evaluate(model, validation_data_loader, device=device)\n",
    "torch.save(model,\"model3SGD\")\n",
    "print(\"Fin de l'entrainement, modele sauvegardé\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8441e1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ZONE DE GUERRE #1 ####\n",
    "\n",
    "transform = A.Compose([\n",
    "    #A.RandomCrop(width=450, height=450),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "], bbox_params=A.BboxParams(format='albumentations', min_area=12, min_visibility=0.1)) #, label_fields=['class_labels']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b50a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = read_file('data3/train/annotations.json')\n",
    "df = extractDataSetFromCOCO(dataset,'data3/train/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be99151",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nbe = 303\n",
    "img = df['image'][nbe]/255\n",
    "(x,y,z) = img.shape\n",
    "df['xmin'] = df['xmin']/x\n",
    "df['xmax'] = df['xmax']/x\n",
    "df['ymin'] = df['ymin']/y\n",
    "df['ymax'] = df['ymax']/y\n",
    "box = list(df[['xmin','ymin','xmax','ymax','classes']].iloc[nbe,:])\n",
    "transformed = transform(image=img, bboxes=[box],class_labels=list(df['classes'].unique())) # , class_categories=class_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dd5c34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transformed['image']  *= 255\n",
    "transformed['image'] = transformed['image'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137378c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image = torch.from_numpy(transformed['image']).to(torch.uint8)\n",
    "box = np.array(transformed['bboxes'][0][0:-1])\n",
    "box[0] *= x\n",
    "box[2] *= x\n",
    "box[1] *= y\n",
    "box[3] *= y\n",
    "print(box)\n",
    "print(image.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d949ba99",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(l_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4373d560",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
