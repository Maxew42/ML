{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cfa7b28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import csv\n",
    "random.seed(4)\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.transforms.functional as F\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.io import read_image\n",
    "from pathlib import Path\n",
    "#import albumentations as A\n",
    "import cv2\n",
    "from helpers.plotting import *\n",
    "from helpers.ml_utils import *\n",
    "## A nettoyer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac466c1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Parametrisation ###\n",
    "\n",
    "model_path = \"../saved_models/model_SGD_balanced_1.2_no_kebab/model_SGD_balanced_1.2_no_kebab\"\n",
    "name_dataset = 'dataset_Trashedy_1.2_augmented'\n",
    "path_to_data = '../dataset/'+ name_dataset + '/valid/'\n",
    "device      = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "cat_to_index = {'other': 1, \n",
    "                'pet': 2,\n",
    "                'plastic_bag' : 3\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3831dd0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tensor_transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "dataset = ObjectDetectionDataset(path_to_dataset = path_to_data,  \n",
    "                                       transform = tensor_transform,\n",
    "                                       mapping = cat_to_index,\n",
    "                                       mode = 'test')\n",
    "\n",
    "data_loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=0, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e19af7f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../saved_models/model_SGD_balanced/model_SGD_balanced_1.2_no_kebab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-f734c043ce43>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\maxim\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    592\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'encoding'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 594\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    595\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    596\u001b[0m             \u001b[1;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\maxim\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'w'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\maxim\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../saved_models/model_SGD_balanced/model_SGD_balanced_1.2_no_kebab'"
     ]
    }
   ],
   "source": [
    "model = torch.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64eb9070",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results = evaluate_to(data_loader,device,cat_to_index,30,model)\n",
    "print(\"Fin de l'évaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76837af5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "l_file_name = np.unique([row['file_name'] for row in results])\n",
    "n_file_name = len(l_file_name)\n",
    "n_objects = len(results)\n",
    "print(\"{} Images avec {} objets detectés\".format(n_file_name,n_objects))\n",
    "idx = int(input(\"Entrer indice de l'image à afficher : \"))\n",
    "color_map = {'other': 'red','pet':'blue','plastic_bag':'orange'}\n",
    "show_result(results,path_to_data , l_file_name[idx],color_map = color_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a02461",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data_2 = '../dataset/'+ name_dataset + '/train/'\n",
    "valid_dataset = ObjectDetectionDataset(path_to_dataset = path_to_data_2,  \n",
    "                                       transform = tensor_transform,\n",
    "                                       mapping = cat_to_index,\n",
    "                                       mode = 'valid')\n",
    "\n",
    "valid_data_loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b8589e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mapping = { value : key for (key, value) in cat_to_index.items()}\n",
    "cat = ['other','pet','plastic bag']\n",
    "nbe = 1\n",
    "\n",
    "targ = valid_dataset.__getitem__(nbe)\n",
    "img = targ[0].permute(1,2,0)\n",
    "l_boxes = targ[1]['boxes']\n",
    "labels = targ[1]['labels']\n",
    "labels = list(labels.numpy())\n",
    "labels = [cat[i-1] for i in labels]\n",
    "scores = [0.1 for i in labels]\n",
    "show_bounding_boxes(img,l_boxes,labels = labels,l_scores = scores,fontsize = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aeb533a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fb7f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbe = 1\n",
    "\n",
    "targ = valid_dataset.__getitem__(nbe)\n",
    "img = targ[0].permute(1,2,0)\n",
    "l_boxes = targ[1]['boxes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0def5a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a= list(l_boxes.numpy())\n",
    "print(a)\n",
    "b = get_intersection_area(a[5],a[6])\n",
    "print(b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
